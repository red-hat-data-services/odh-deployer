apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus
  name: prometheus
  namespace: redhat-ods-monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      deployment: prometheus
  template:
    metadata:
      labels:
        deployment: prometheus
    spec:
      serviceAccountName: prometheus
      initContainers:
        - name: wait-for-deployment
          image: 'registry.access.redhat.com/ubi8/ubi-minimal:8.4-208'
          command:
            - /bin/sh
            - '-c'
            - for i in `seq 1 230`; do sleep 10; echo "Waiting for traefik-proxy service to become available..."; if curl -s http://traefik-proxy.redhat-ods-applications.svc:8080; then exit 0; fi; done; exit 1
      containers:
      - name: oauth-proxy
        args:
        - -provider=openshift
        - -https-address=:9091
        - -http-address=
        - -email-domain=*
        - -upstream=http://localhost:9090
        - -openshift-service-account=prometheus
        - '-openshift-sar={"resource": "namespaces", "verb": "get", "name": "redhat-ods-monitoring",
          "namespace": "redhat-ods-monitoring"}'
        - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get",
          "name": "redhat-ods-monitoring", "namespace": "redhat-ods-monitoring"}}'
        - -tls-cert=/etc/tls/private/tls.crt
        - -tls-key=/etc/tls/private/tls.key
        - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
        - -cookie-secret-file=/etc/proxy/secrets/session_secret
        - -openshift-ca=/etc/pki/tls/cert.pem
        - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        - -client-id=system:serviceaccount:redhat-ods-monitoring:prometheus
        - -skip-auth-regex=^/metrics
        image: quay.io/openshift/origin-oauth-proxy:4.7.0
        ports:
        - containerPort: 9091
          name: https
        resources:
          limits:
            cpu: 100m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 256Mi
        volumeMounts:
        - mountPath: /etc/tls/private
          name: prometheus-tls
          readOnly: false
        - mountPath: /etc/proxy/secrets
          name: prometheus-proxy
          readOnly: false

      - name: prometheus
        image: quay.io/prometheus/prometheus:v2.24.1
        args:
          - --storage.tsdb.retention.time=6h
          - --storage.tsdb.min-block-duration=2h
          - --storage.tsdb.max-block-duration=2h
          - --storage.tsdb.path=/prometheus/data
          - --config.file=/etc/prometheus/prometheus.yml
          - --web.listen-address=localhost:9090
          - --web.enable-lifecycle
          - --web.enable-admin-api
        imagePullPolicy: Always
        ports:
        - containerPort: 9090
          name: http
        resources:
          limits:
            cpu: 400m
            memory: 4Gi
          requests:
            cpu: 200m
            memory: 2Gi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /prometheus
          name: prometheus-data
        - mountPath: /etc/prometheus
          name: prometheus-config
        - mountPath: /var/run/secrets/kubernetes.io/scraper
          name: prometheus-secret

      - name: alertmanager-proxy
        args:
        - -provider=openshift
        - -https-address=:10443
        - -http-address=
        - -email-domain=*
        - -upstream=http://localhost:9093
        - -openshift-service-account=prometheus
        - '-openshift-sar={"resource": "namespaces", "verb": "get", "name": "redhat-ods-monitoring",
          "namespace": "redhat-ods-monitoring"}'
        - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get",
          "name": "redhat-ods-monitoring", "namespace": "redhat-ods-monitoring"}}'
        - -tls-cert=/etc/tls/private/tls.crt
        - -tls-key=/etc/tls/private/tls.key
        - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
        - -cookie-secret-file=/etc/proxy/secrets/session_secret
        - -openshift-ca=/etc/pki/tls/cert.pem
        - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        - -client-id=system:serviceaccount:redhat-ods-monitoring:prometheus
        - -skip-auth-regex=^/metrics
        image: quay.io/openshift/origin-oauth-proxy:4.7.0
        imagePullPolicy: Always
        ports:
        - containerPort: 10443
          name: web
        resources:
          limits:
            cpu: 100m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 256Mi
        volumeMounts:
        - mountPath: /etc/tls/private
          name: alertmanager-tls
          readOnly: false
        - mountPath: /etc/proxy/secrets
          name: alertmanager-proxy
          readOnly: false

      - name: alertmanager
        args:
          - --log.level=info
          - --config.file=/etc/alertmanager/alertmanager.yml
          - --web.external-url=https://<set_alertmanager_host>
        image: quay.io/prometheus/alertmanager:v0.21.0
        imagePullPolicy: Always
        ports:
          - containerPort: 9093
            name: web
        volumeMounts:
          - mountPath: /etc/alertmanager
            name: alertmanager-config
          - mountPath: /alertmanager
            name: alertmanager-data

      dnsPolicy: ClusterFirst
      restartPolicy: Always
      progressDeadlineSeconds: 2400
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 90
      volumes:
      - name: prometheus-data
        persistentVolumeClaim:
          claimName: prometheus-data
      - name: alertmanager-data
        persistentVolumeClaim:
          claimName: "alertmanager-data"
      - name: alertmanager-config
        configMap:
          defaultMode: 420
          name: alertmanager
      - name: prometheus-config
        configMap:
          defaultMode: 420
          name: prometheus
      - name: prometheus-secret
        secret:
          secretName: prometheus-secret
      - name: prometheus-tls
        secret:
          defaultMode: 420
          secretName: prometheus-tls
      - name: alertmanager-tls
        secret:
          defaultMode: 420
          secretName: alertmanager-tls
      - name: prometheus-proxy
        secret:
          defaultMode: 420
          secretName: prometheus-proxy
      - name: alertmanager-proxy
        secret:
          defaultMode: 420
          secretName: alertmanager-proxy

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus
  namespace: redhat-ods-monitoring
data:
  recording.rules: |
    groups:
      - name: SLOs - JupyterHub
        rules:
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[1d]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[1d]))
          labels:
            route: jupyterhub
          record: haproxy_backend_http_responses_total:burnrate1d
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[1h]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[1h]))
          labels:
            route: jupyterhub
          record: haproxy_backend_http_responses_total:burnrate1h
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[2h]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[2h]))
          labels:
            route: jupyterhub
          record: haproxy_backend_http_responses_total:burnrate2h
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[30m]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[30m]))
          labels:
            route: jupyterhub
          record: haproxy_backend_http_responses_total:burnrate30m
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[3d]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[3d]))
          labels:
            route: jupyterhub
          record: haproxy_backend_http_responses_total:burnrate3d
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[5m]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[5m]))
          labels:
            route: jupyterhub
          record: haproxy_backend_http_responses_total:burnrate5m
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub",code=~"5.."}[6h]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="jupyterhub"}[6h]))
          labels:
            route: jupyterhub
          record: haproxy_backend_http_responses_total:burnrate6h

        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[1d])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate1d
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[1h])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate1h
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[2h])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate2h
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[30m])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate30m
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[3d])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate3d
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[5m])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate5m
        - expr: |
            1 - avg_over_time(probe_success{instance=~"jupyterhub-.*", job="user_facing_endpoints_status"}[6h])
          labels:
            instance: jupyterhub
          record: probe_success:burnrate6h

      - name: SLOs - Traefik Proxy
        rules:
        - expr: min(up{job="Traefik Proxy Metrics"})
          labels:
            instance: traefik
            job: user_facing_endpoints_status
          record:
            probe_success
        - expr: |
            1 - avg_over_time(probe_success{instance=~"traefik", job="user_facing_endpoints_status"}[1d])
          labels:
            instance: traefik
          record: probe_success:burnrate1d
        - expr: |
            1 - avg_over_time(probe_success{instance=~"traefik", job="user_facing_endpoints_status"}[1h])
          labels:
            instance: traefik
          record: probe_success:burnrate1h
        - expr: |
            1 - avg_over_time(probe_success{instance=~"traefik", job="user_facing_endpoints_status"}[2h])
          labels:
            instance: traefik
          record: probe_success:burnrate2h
        - expr: |
            1 - avg_over_time(probe_success{instance=~"traefik", job="user_facing_endpoints_status"}[30m])
          labels:
            instance: traefik
          record: probe_success:burnrate30m
        - expr: |
            1 - avg_over_time(probe_success{instance=~"traefik", job="user_facing_endpoints_status"}[3d])
          labels:
            instance: traefik
          record: probe_success:burnrate3d
        - expr: |
            1 - avg_over_time(probe_success{instance=~"traefik", job="user_facing_endpoints_status"}[5m])
          labels:
            instance: traefik
          record: probe_success:burnrate5m
        - expr: |
            1 - avg_over_time(probe_success{instance=~"traefik", job="user_facing_endpoints_status"}[6h])
          labels:
            instance: traefik
          record: probe_success:burnrate6h

      - name: SLOs - ODH Dashboard
        rules:
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard",code=~"5.."}[1d]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard"}[1d]))
          labels:
            route: odh-dashboard
          record: haproxy_backend_http_responses_total:burnrate1d
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard",code=~"5.."}[1h]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard"}[1h]))
          labels:
            route: odh-dashboard
          record: haproxy_backend_http_responses_total:burnrate1h
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard",code=~"5.."}[2h]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard"}[2h]))
          labels:
            route: odh-dashboard
          record: haproxy_backend_http_responses_total:burnrate2h
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard",code=~"5.."}[30m]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard"}[30m]))
          labels:
            route: odh-dashboard
          record: haproxy_backend_http_responses_total:burnrate30m
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard",code=~"5.."}[3d]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard"}[3d]))
          labels:
            route: odh-dashboard
          record: haproxy_backend_http_responses_total:burnrate3d
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard",code=~"5.."}[5m]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard"}[5m]))
          labels:
            route: odh-dashboard
          record: haproxy_backend_http_responses_total:burnrate5m
        - expr: |
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard",code=~"5.."}[6h]))
            /
            sum(rate(haproxy_backend_http_responses_total{route="odh-dashboard"}[6h]))
          labels:
            route: odh-dashboard
          record: haproxy_backend_http_responses_total:burnrate6h

        - expr: |
            1 - avg_over_time(probe_success{instance=~"odh-dashboard-.*", job="user_facing_endpoints_status"}[1d])
          labels:
            instance: odh-dashboard
          record: probe_success:burnrate1d
        - expr: |
            1 - avg_over_time(probe_success{instance=~"odh-dashboard-.*", job="user_facing_endpoints_status"}[1h])
          labels:
            instance: odh-dashboard
          record: probe_success:burnrate1h
        - expr: |
            1 - avg_over_time(probe_success{instance=~"odh-dashboard-.*", job="user_facing_endpoints_status"}[2h])
          labels:
            instance: odh-dashboard
          record: probe_success:burnrate2h
        - expr: |
            1 - avg_over_time(probe_success{instance=~"odh-dashboard-.*", job="user_facing_endpoints_status"}[30m])
          labels:
            instance: odh-dashboard
          record: probe_success:burnrate30m
        - expr: |
            1 - avg_over_time(probe_success{instance=~"odh-dashboard-.*", job="user_facing_endpoints_status"}[3d])
          labels:
            instance: odh-dashboard
          record: probe_success:burnrate3d
        - expr: |
            1 - avg_over_time(probe_success{instance=~"odh-dashboard-.*", job="user_facing_endpoints_status"}[5m])
          labels:
            instance: odh-dashboard
          record: probe_success:burnrate5m
        - expr: |
            1 - avg_over_time(probe_success{instance=~"odh-dashboard-.*", job="user_facing_endpoints_status"}[6h])
          labels:
            instance: odh-dashboard
          record: probe_success:burnrate6h

      - name: SLOs - JupyterHub DB
        rules:
        - expr: |
            1 - avg_over_time(jupyterhub_db_probe_success[1d])
          labels:
            instance: jupyterhub-db
          record: probe_success:burnrate1d
        - expr: |
            1 - avg_over_time(jupyterhub_db_probe_success[1h])
          labels:
            instance: jupyterhub-db
          record: probe_success:burnrate1h
        - expr: |
            1 - avg_over_time(jupyterhub_db_probe_success[2h])
          labels:
            instance: jupyterhub-db
          record: probe_success:burnrate2h
        - expr: |
            1 - avg_over_time(jupyterhub_db_probe_success[30m])
          labels:
            instance: jupyterhub-db
          record: probe_success:burnrate30m
        - expr: |
            1 - avg_over_time(jupyterhub_db_probe_success[3d])
          labels:
            instance: jupyterhub-db
          record: probe_success:burnrate3d
        - expr: |
            1 - avg_over_time(jupyterhub_db_probe_success[5m])
          labels:
            instance: jupyterhub-db
          record: probe_success:burnrate5m
        - expr: |
            1 - avg_over_time(jupyterhub_db_probe_success[6h])
          labels:
            instance: jupyterhub-db
          record: probe_success:burnrate6h

      - name: SLOs - RHODS Operator
        interval: 15m
        rules:
        - expr: |
            rate(controller_runtime_reconcile_total{controller="kfdef-controller", job="RHODS Metrics", result!="success"}[15m])
          labels:
            instance: rhods-controller
          record: controller_runtime_reconcile_total:rate15m

      - name: Usage Metrics
        rules:
        - expr: max(jupyterhub_total_users)
          labels:
            instance: jupyterhub
          record: rhods_total_users

      - name: Availability Metrics
        rules:
        - expr: min(probe_success)
          record: rhods_aggregate_availability

  alerting.rules: |
    groups:
      - name: Pod Restarts
        interval: 1m
        rules:
          - alert: An OpenShift pod has just restarted
            expr: sum(delta(kube_pod_container_status_restarts_total{namespace="rhods-notebooks", pod!~"jupyterhub-nb-.*"}[5m])) by (namespace, pod) > 0
            for: 5m
            annotations:
              summary: An OpenShift pod has just been restarted.
              action: Check {{ $labels.kubernetes_pod_name }} pod in {{ $labels.kubernetes_namespace}} namespace to determine why the pod is restarting.
              triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"

      - name: DeadManSnitch
        interval: 1m
        rules:
          - alert: DeadManSnitch
            expr: vector(1)
            labels:
              severity: critical
            annotations:
              description: This is a DeadManSnitch to ensure RHODS monitoring and alerting pipeline is online.
              summary: Alerting DeadManSnitch

      - name: SLOs-haproxy_backend_http_responses_total
        rules:
        - alert: RHODS Route Error Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.route }} (current value: {{ $value }})'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            sum(haproxy_backend_http_responses_total:burnrate5m{route=~"jupyterhub|odh-dashboard"}) by (route) > (14.40 * (1-0.98000))
            and
            sum(haproxy_backend_http_responses_total:burnrate1h{route=~"jupyterhub|odh-dashboard"}) by (route) > (14.40 * (1-0.98000))
          for: 2m
          labels:
            severity: critical
        - alert: RHODS Route Error Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.route }} (current value: {{ $value }})'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            sum(haproxy_backend_http_responses_total:burnrate30m{route=~"jupyterhub|odh-dashboard"}) by (route) > (6.00 * (1-0.98000))
            and
            sum(haproxy_backend_http_responses_total:burnrate6h{route=~"jupyterhub|odh-dashboard"}) by (route) > (6.00 * (1-0.98000))
          for: 15m
          labels:
            severity: critical
        - alert: RHODS Route Error Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.route }} (current value: {{ $value }})'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            sum(haproxy_backend_http_responses_total:burnrate2h{route=~"jupyterhub|odh-dashboard"}) by (route) > (3.00 * (1-0.98000))
            and
            sum(haproxy_backend_http_responses_total:burnrate1d{route=~"jupyterhub|odh-dashboard"}) by (route) > (3.00 * (1-0.98000))
          for: 1h
          labels:
            severity: warning
        - alert: RHODS Route Error Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.route }} (current value: {{ $value }})'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            sum(haproxy_backend_http_responses_total:burnrate6h{route=~"jupyterhub|odh-dashboard"}) by (route) > (1.00 * (1-0.98000))
            and
            sum(haproxy_backend_http_responses_total:burnrate3d{route=~"jupyterhub|odh-dashboard"}) by (route) > (1.00 * (1-0.98000))
          for: 3h
          labels:
            severity: warning

      - name: SLOs-probe_success
        rules:
        - alert: RHODS Probe Success Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.instance }} (current value: {{ $value }})'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            sum(probe_success:burnrate5m{instance=~"jupyterhub|odh-dashboard|jupyterhub-db|traefik"}) by (instance) > (14.40 * (1-0.98000))
            and
            sum(probe_success:burnrate1h{instance=~"jupyterhub|odh-dashboard|jupyterhub-db|traefik"}) by (instance) > (14.40 * (1-0.98000))
          for: 2m
          labels:
            severity: critical
        - alert: RHODS Probe Success Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.instance }} (current value: {{ $value }})'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            sum(probe_success:burnrate30m{instance=~"jupyterhub|odh-dashboard|jupyterhub-db|traefik"}) by (instance) > (6.00 * (1-0.98000))
            and
            sum(probe_success:burnrate6h{instance=~"jupyterhub|odh-dashboard|jupyterhub-db|traefik"}) by (instance) > (6.00 * (1-0.98000))
          for: 15m
          labels:
            severity: critical
        - alert: RHODS Probe Success Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.instance }} (current value: {{ $value }})'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            sum(probe_success:burnrate2h{instance=~"jupyterhub|odh-dashboard|jupyterhub-db|traefik"}) by (instance) > (3.00 * (1-0.98000))
            and
            sum(probe_success:burnrate1d{instance=~"jupyterhub|odh-dashboard|jupyterhub-db|traefik"}) by (instance) > (3.00 * (1-0.98000))
          for: 1h
          labels:
            severity: warning
        - alert: RHODS Probe Success Burn Rate
          annotations:
            message: 'High error budget burn for {{ $labels.instance }} (current value: {{ $value }})'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            sum(probe_success:burnrate6h{instance=~"jupyterhub|odh-dashboard|jupyterhub-db|traefik"}) by (instance) > (1.00 * (1-0.98000))
            and
            sum(probe_success:burnrate3d{instance=~"jupyterhub|odh-dashboard|jupyterhub-db|traefik"}) by (instance) > (1.00 * (1-0.98000))
          for: 3h
          labels:
            severity: warning

      - name: SLOs-controller_runtime_reconcile_total
        rules:
        - alert: RHODS Operator Reconciliation Failures
          annotations:
            message: 'The RHODS operator has failed to reconcile successfully for the last hour'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
           min_over_time(controller_runtime_reconcile_total:rate15m{instance="rhods-controller"}[1h])
          for: 2m
          labels:
            severity: critical
        - alert: RHODS Operator Reconciliation Failures
          annotations:
            message: 'The RHODS operator has failed to reconcile successfully for the last 30 minutes'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
          expr: |
            min_over_time(controller_runtime_reconcile_total:rate15m{instance="rhods-controller"}[30m])
          for: 2m
          labels:
            severity: warning

      - name: Builds
        rules:
        - alert: JupyterHub image builds are failing
          annotations:
            message: 'A JupyterHub image build is in a failed state'
            triage: "https://gitlab.cee.redhat.com/service/managed-tenants-sops/-/tree/master/RHODS/README.md"
            build: "{{ $labels.buildconfig }}"
          expr: openshift_build_status_phase_total{build_phase="failed", namespace="redhat-ods-applications"} == 1
          for: 15m
          labels:
            severity: critical

  prometheus.yml: |
    rule_files:
      - '*.rules'
    global:
      scrape_interval:     10s
      evaluation_interval: 10s

    scrape_configs:
    - job_name: 'Federate Prometheus'
      scrape_interval: 30s
      scheme: https
      tls_config:
        insecure_skip_verify: true
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token: "<prom_bearer_token>"
      honor_labels: true
      metrics_path: '/federate'

      params:
        'match[]':
          - '{__name__= "haproxy_backend_http_responses_total"}'
          - '{__name__= "controller_runtime_reconcile_total"}'
          - '{__name__= "container_cpu_usage_seconds_total"}'
          - '{__name__= "container_memory_rss"}'
          - '{__name__= "kubelet_volume_stats_used_bytes"}'
          - '{__name__= "kubelet_volume_stats_capacity_bytes"}'
          - '{__name__= "kube_pod_container_status_waiting_reason"}'
          - '{__name__= "kube_pod_container_status_restarts_total"}'
          - '{__name__= "kube_pod_container_status_terminated_reason"}'
          - '{__name__= "openshift_build_status_phase_total"}'

      static_configs:
        - targets:
          - "<federate_target>"

    - job_name: 'JupyterHub Metrics'
      honor_labels: true
      metrics_path: /hub/metrics
      scheme: http
      bearer_token: <jupyterhub_prometheus_api_token>
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
              - redhat-ods-applications
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_name]
          regex: ^(jupyterhub)$
          target_label: kubernetes_name
          action: keep
        - source_labels: [__address__]
          regex: (.+):(\d+)
          target_label: __address__
          replacement: ${1}:8081

    - job_name: 'Traefik Proxy Metrics'
      honor_labels: true
      metrics_path: /metrics
      scheme: http
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
              - redhat-ods-applications
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_name]
          regex: ^(traefik-proxy)$
          target_label: kubernetes_name
          action: keep
        - source_labels: [__address__]
          regex: (.+):(\d+)
          target_label: __address__
          replacement: ${1}:8082

    - job_name: 'user_facing_endpoints_status'
      scrape_interval: 10s
      metrics_path: /probe
      params:
        module: [http_2xx]
      static_configs:
      - targets:
        - <jupyterhub_host>
        - <rhods_dashboard_host>
      relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter.redhat-ods-monitoring.svc.cluster.local:9115

    - job_name: 'RHODS Metrics'
      honor_labels: true
      scheme: http
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
              - redhat-ods-operator
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_name]
          regex: ^(rhods-operator-metrics)$
          target_label: kubernetes_name
          action: keep
        - source_labels: [__address__]
          regex: (.+):(\d+)
          target_label: __address__
          replacement: ${1}:8383

    - job_name: "Jupyterhub DB Metrics"
      honor_labels: true
      scheme: http
      metrics_path: /
      kubernetes_sd_configs:
        - role: endpoints
          namespaces:
            names:
              - redhat-ods-applications
      relabel_configs:
        - source_labels: [__meta_kubernetes_service_name]
          regex: ^(jupyterhub-db-probe)$
          target_label: kubernetes_name
          action: keep
        - source_labels: [__address__]
          regex: (.+):(\d+)
          target_label: __address__
          replacement: ${1}:8080

    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "localhost:9093"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager
  namespace: redhat-ods-monitoring
data:
  alertmanager.yml: |
    global:
      smtp_from: 'noreply-alert@redhat.com'
      smtp_smarthost: '<smtp_host>:<smtp_port>'
      smtp_auth_username: '<smtp_username>'
      smtp_auth_password: '<smtp_password>'
      smtp_require_tls: true

    # The root route on which each incoming alert enters.
    route:
      group_by: ['alertname', 'cluster', 'service', 'job', 'email_to']

      group_wait: 30s

      # When the first notification was sent, wait 'group_interval' to send a batch
      # of new alerts that started firing for that group.
      group_interval: 5m

      # If an alert has successfully been sent, wait 'repeat_interval' to
      # resend them.
      repeat_interval: 4h

      # A default receiver
      receiver: PagerDuty

      routes:
      - match:
          alertname: DeadManSnitch
        receiver: deadman-snitch
        repeat_interval: 5m

    receivers:
    - name: 'developer-mails'
      email_configs:
      - to: 'test-123@foo.com'
        send_resolved: true

    - name: 'user-notifications'
      email_configs:
      - to: '<user_emails>'
        send_resolved: false

    - name: 'PagerDuty'
      pagerduty_configs:
      - service_key: <pagerduty_token>
        send_resolved: true

    - name: 'deadman-snitch'
      webhook_configs:
        - url: '<snitch_url>?m=just+checking+in'
          send_resolved: false

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: alertmanager-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

---
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.alpha.openshift.io/serving-cert-secret-name: prometheus-tls
    prometheus.io/scheme: https
    prometheus.io/scrape: 'true'
  labels:
    app: prometheus
  name: prometheus
  namespace: redhat-ods-monitoring
spec:
  ports:
  - name: https
    port: 9091
    targetPort: https
  selector:
    deployment: prometheus
  sessionAffinity: None
  type: ClusterIP

---
# Create a service account for accessing prometheus data
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus-reader
  namespace: redhat-ods-monitoring
---
apiVersion: v1
kind: Secret
metadata:
  name: prometheus-secret
  namespace: redhat-ods-monitoring
  annotations:
    kubernetes.io/service-account.name: prometheus
type: kubernetes.io/service-account-token
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    kubernetes.io/tls-acme: 'true'
  labels:
    app: prometheus
  name: prometheus
  namespace: redhat-ods-monitoring
spec:
  port:
    targetPort: https
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: reencrypt
  to:
    kind: Service
    name: prometheus
  wildcardPolicy: None
---
apiVersion: authorization.openshift.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus-scraper
roleRef:
  name: prometheus-scraper
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: redhat-ods-monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-scraper
rules:
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get
- apiGroups:
  - route.openshift.io
  resources:
  - routers/metrics
  - routers/federate
  verbs:
  - get
- apiGroups:
  - image.openshift.io
  resources:
  - registry/metrics
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - endpoints
  - pods
  - services
  - namespaces
  verbs:
  - list
  - get
  - watch
